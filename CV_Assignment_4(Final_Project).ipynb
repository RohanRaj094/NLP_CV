{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanRaj094/NLP_CV/blob/main/CV_Assignment_4(Final_Project).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0hVeIT4GMK4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF8nZ_h6N_9J"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 1: Install All Libraries**\n",
        "This cell installs all the libraries we need.\n",
        "1. <blue>**fiftyone**</blue>: The library we use to download the <green>COCO dataset</green>.\n",
        "2. <blue>**segment-anything**</blue>: The official library from Meta to run the <green>SAM model</green>.\n",
        "3. <blue>**pycocotools**</blue>: A helper library that <green>fiftyone</green> needs to load COCO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKfwbHoeOgJn",
        "outputId": "dac1b7a5-40e6-4a71-a058-f313a60de912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.7/308.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.0/97.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m429.9/429.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mcp 1.19.0 requires sse-starlette>=1.6.1, but you have sse-starlette 0.10.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "All libraries installed.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q fiftyone supervision\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q pycocotools\n",
        "\n",
        "print(\"All libraries installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXVp291LQ3Z8"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 2: Import Libraries & Set Device**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imr-xW5kQ9K4",
        "outputId": "e81b5ca7-19a7-4b51-9db3-b84fe39064cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n",
            "  return '(?ms)' + res + '\\Z'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ08FpvKRM6I"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 3: Download Dataset**\n",
        "1. <blue>**foz.load_zoo_dataset**</blue>: This function downloads the <green>validation split</green> of COCO.\n",
        "2. <blue>**max_samples=500**</blue>: Dataset size to <green>500 images</green>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxeveN4vRVz2",
        "outputId": "f55b9a94-2cf6-4c7d-98ff-6b63e38e03ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.core.odm.database:You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████|    1.9Gb/1.9Gb [5.0s elapsed, 0s remaining, 436.9Mb/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    1.9Gb/1.9Gb [5.0s elapsed, 0s remaining, 436.9Mb/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 500 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading 500 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████████████████| 500/500 [1.6m elapsed, 0s remaining, 5.7 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████████████████| 500/500 [1.6m elapsed, 0s remaining, 5.7 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations for 500 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 500 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 500/500 [9.8s elapsed, 0s remaining, 66.0 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 500/500 [9.8s elapsed, 0s remaining, 66.0 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'coco-2017-validation-500' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-validation-500' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 images.\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"coco-2017-validation-500\"\n",
        "\n",
        "if fo.dataset_exists(dataset_name):\n",
        "    fo.delete_dataset(dataset_name)\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"segmentations\"],\n",
        "    max_samples=500,\n",
        "    dataset_name=dataset_name\n",
        ")\n",
        "dataset.persistent = True\n",
        "print(f\"Loaded {len(dataset)} images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HXH9lK9R22W"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 4: Load Models**\n",
        "1. <blue>**Mask R-CNN**</blue>: We load the <green>pre-trained</green> model from `torchvision`.\n",
        "2. <blue>**SAM**</blue>: We download the official <green>SAM checkpoint file</green> and use the `sam_model_registry`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4WLeVp3SAE9",
        "outputId": "7f5d6193-7fcd-43de-981a-c9e978ea4f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 172MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SAM checkpoint...\n",
            "Download complete.\n",
            "Both models are loaded.\n"
          ]
        }
      ],
      "source": [
        "mask_rcnn_model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT').to(DEVICE)\n",
        "mask_rcnn_model.eval()\n",
        "\n",
        "checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "checkpoint_path = \"sam_vit_b_01ec64.pth\"\n",
        "\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(\"Downloading SAM checkpoint...\")\n",
        "    r = requests.get(checkpoint_url)\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "model_type = \"vit_b\"\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "sam_model.to(device=DEVICE)\n",
        "\n",
        "sam_mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
        "\n",
        "print(\"Both models are loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN5ppoD8QN9V"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 5: Helper Functions**\n",
        "This block defines small functions we'll need in the main loop.\n",
        "1. <blue>**preprocess**</blue>: Prepares images for the <green>Mask R-CNN</green> model.\n",
        "2. <blue>**get_ground_truth_masks**</blue>: Gets the <green>correct \"answer\" masks</green> from the dataset and stacks them into a single tensor.\n",
        "3. <blue>**calculate_dice_score**</blue>: The function to measure <green>mask overlap</green>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAZL73f5QV-Q",
        "outputId": "d1e8293c-aee1-47d2-abd0-24b9445137e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ],
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "COCO_CLASSES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "def calculate_dice_score(mask1, mask2):\n",
        "    mask1 = mask1.bool()\n",
        "    mask2 = mask2.bool()\n",
        "\n",
        "    intersection = (mask1 & mask2).sum().float()\n",
        "    union = mask1.sum().float() + mask2.sum().float()\n",
        "\n",
        "    if union == 0:\n",
        "        return 1.0\n",
        "\n",
        "    dice = (2. * intersection) / (union + 1e-8)\n",
        "    return dice.item()\n",
        "\n",
        "def get_ground_truth_masks(sample):\n",
        "    img_height = sample.metadata.height\n",
        "    img_width = sample.metadata.width\n",
        "    gt_masks = []\n",
        "\n",
        "    if sample.ground_truth is None:\n",
        "        return torch.empty(0, img_height, img_width)\n",
        "\n",
        "    for detection in sample.ground_truth.detections:\n",
        "        if detection.mask is not None:\n",
        "            full_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "            cropped_mask = detection.mask\n",
        "            h_crop, w_crop = cropped_mask.shape\n",
        "\n",
        "            x, y, w, h = detection.bounding_box\n",
        "            xmin = int(x * img_width)\n",
        "            ymin = int(y * img_height)\n",
        "\n",
        "            ymax = ymin + h_crop\n",
        "            xmax = xmin + w_crop\n",
        "\n",
        "            # Check for rounding errors at the image boundary\n",
        "            if ymax > img_height:\n",
        "                ymax = img_height\n",
        "            if xmax > img_width:\n",
        "                xmax = img_width\n",
        "\n",
        "            # Slice the cropped mask if it goes over the boundary\n",
        "            paste_h = ymax - ymin\n",
        "            paste_w = xmax - xmin\n",
        "\n",
        "            full_mask[ymin:ymax, xmin:xmax] = cropped_mask[0:paste_h, 0:paste_w]\n",
        "\n",
        "            gt_masks.append(torch.from_numpy(full_mask))\n",
        "\n",
        "    if not gt_masks:\n",
        "        return torch.empty(0, img_height, img_width)\n",
        "\n",
        "    return torch.stack(gt_masks)\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvYr19_Qehq"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 6: Run Evaluation Loop**\n",
        " It loops over all <green>**500 images**</green> to run the comparison.\n",
        "1. <blue>**For loop**</blue>: Iterates through each `sample` in our `dataset`.\n",
        "2. <blue>**Mask R-CNN / SAM**</blue>: Runs each model on the image, <green>times the speed</green>, and calculates the `dice_score`.\n",
        "3. <blue>**Results**</blue>: At the end, it prints the <green>final average Dice Score</green> and <green>Inference Speed (FPS)</green>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6dg4sNpQpTB",
        "outputId": "66baca9f-cf7e-4c30-89d4-81b677deebff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 images to evaluate.\n",
            "Starting evaluation loop...\n",
            "  16% |██\\--------------|  79/500 [7.2m elapsed, 38.7m remaining, 0.2 samples/s]  "
          ]
        }
      ],
      "source": [
        "results = []\n",
        "mask_rcnn_total_time = 0\n",
        "sam_total_time = 0\n",
        "\n",
        "num_images = len(dataset)\n",
        "print(f\"Found {num_images} images to evaluate.\")\n",
        "\n",
        "print(\"Starting evaluation loop...\")\n",
        "\n",
        "with fo.ProgressBar() as pb:\n",
        "    for sample in dataset.iter_samples(autosave=True, progress=pb):\n",
        "        image_path = sample.filepath\n",
        "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "        image_np = np.array(image_pil)\n",
        "\n",
        "        gt_masks = get_ground_truth_masks(sample).to(DEVICE)\n",
        "\n",
        "        image_tensor = preprocess(image_pil).to(DEVICE)\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            prediction = mask_rcnn_model([image_tensor])\n",
        "        mask_rcnn_total_time += (time.time() - start_time)\n",
        "\n",
        "        pred_masks = prediction[0]['masks'][:, 0, :, :] > 0.5\n",
        "\n",
        "        avg_dice_rcnn = 0\n",
        "        if gt_masks.shape[0] > 0 and pred_masks.shape[0] > 0:\n",
        "            dice_scores = []\n",
        "            for gt_mask in gt_masks:\n",
        "                best_dice = 0\n",
        "                for pred_mask in pred_masks:\n",
        "                    dice = calculate_dice_score(gt_mask, pred_mask)\n",
        "                    if dice > best_dice:\n",
        "                        best_dice = dice\n",
        "                dice_scores.append(best_dice)\n",
        "            avg_dice_rcnn = np.mean(dice_scores) if dice_scores else 0\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "        outputs = sam_mask_generator.generate(image_np)\n",
        "        sam_total_time += (time.time() - start_time)\n",
        "\n",
        "        sam_masks = [torch.from_numpy(mask['segmentation']).to(DEVICE) for mask in outputs]\n",
        "        if sam_masks:\n",
        "            sam_masks_tensor = torch.stack(sam_masks)\n",
        "        else:\n",
        "            sam_masks_tensor = torch.empty(0, image_np.shape[0], image_np.shape[1]).to(DEVICE)\n",
        "\n",
        "        avg_dice_sam = 0\n",
        "        if gt_masks.shape[0] > 0 and sam_masks_tensor.shape[0] > 0:\n",
        "            dice_scores = []\n",
        "            for gt_mask in gt_masks:\n",
        "                best_dice = 0\n",
        "                for pred_mask in sam_masks_tensor:\n",
        "                    dice = calculate_dice_score(gt_mask, pred_mask)\n",
        "                    if dice > best_dice:\n",
        "                        best_dice = dice\n",
        "                dice_scores.append(best_dice)\n",
        "            avg_dice_sam = np.mean(dice_scores) if dice_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"image_id\": sample.id,\n",
        "            \"rcnn_dice\": avg_dice_rcnn,\n",
        "            \"sam_dice\": avg_dice_sam\n",
        "        })\n",
        "\n",
        "print(\"Evaluation complete.\")\n",
        "\n",
        "avg_rcnn_dice = np.mean([r['rcnn_dice'] for r in results]) if results else 0\n",
        "avg_sam_dice = np.mean([r['sam_dice'] for r in results]) if results else 0\n",
        "avg_rcnn_fps = num_images / mask_rcnn_total_time if mask_rcnn_total_time > 0 else 0\n",
        "avg_sam_fps = num_images / sam_total_time if sam_total_time > 0 else 0\n",
        "\n",
        "print(\"\\n--- Overall Results ---\")\n",
        "print(f\"Mask R-CNN Avg. Dice Score: {avg_rcnn_dice:.4f}\")\n",
        "print(f\"SAM Avg. Dice Score:          {avg_sam_dice:.4f}\")\n",
        "print(f\"Mask R-CNN Avg. FPS: {avg_rcnn_fps:.2f} (Inference Speed)\")\n",
        "print(f\"SAM Avg. FPS:          {avg_sam_fps:.2f} (Inference Speed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 7: Visualize the Results**"
      ],
      "metadata": {
        "id": "8CgNvYbnAwZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sample_id = random.choice([r['image_id'] for r in results])\n",
        "sample = dataset[sample_id]\n",
        "image_path = sample.filepath\n",
        "image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "image_np = np.array(image_pil)\n",
        "\n",
        "print(f\"Visualizing results for image: {sample.filepath}\")\n",
        "\n",
        "gt_masks = get_ground_truth_masks(sample).cpu().numpy()\n",
        "\n",
        "image_tensor = preprocess(image_pil).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    prediction = mask_rcnn_model([image_tensor])\n",
        "\n",
        "rcnn_masks = prediction[0]['masks'][:, 0, :, :].cpu().numpy() > 0.5\n",
        "rcnn_labels = [COCO_CLASSES[label] for label in prediction[0]['labels'].cpu().numpy()]\n",
        "rcnn_scores = prediction[0]['scores'].cpu().numpy()\n",
        "\n",
        "score_threshold = 0.7\n",
        "rcnn_masks = rcnn_masks[rcnn_scores > score_threshold]\n",
        "rcnn_labels = [label for i, label in enumerate(rcnn_labels) if rcnn_scores[i] > score_threshold]\n",
        "\n",
        "outputs = sam_mask_generator.generate(image_np)\n",
        "sam_masks = [mask['segmentation'] for mask in outputs]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
        "\n",
        "axes[0].imshow(image_np)\n",
        "if gt_masks.shape[0] > 0:\n",
        "    for mask in gt_masks:\n",
        "        sv.plot_mask(ax=axes[0], mask=mask, random_color=True)\n",
        "axes[0].set_title(\"Ground Truth\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(image_np)\n",
        "if rcnn_masks.shape[0] > 0:\n",
        "    for i, mask in enumerate(rcnn_masks):\n",
        "        sv.plot_mask(ax=axes[1], mask=mask, random_color=True)\n",
        "        y, x = np.where(mask)\n",
        "        if len(y) > 0:\n",
        "             axes[1].text(np.mean(x), np.mean(y), rcnn_labels[i], color='white', fontsize=9, bbox=dict(facecolor='black', alpha=0.5))\n",
        "axes[1].set_title(f\"Mask R-CNN (Score > 0.7)\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(image_np)\n",
        "if sam_masks:\n",
        "    for mask in sam_masks:\n",
        "        sv.plot_mask(ax=axes[2], mask=mask, random_color=True)\n",
        "axes[2].set_title(f\"SAM (Zero-Shot)\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cLCRN2jkBCYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QrxXsO0FBGrO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMGd6ebDfSBRDA5da8HvuWX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}